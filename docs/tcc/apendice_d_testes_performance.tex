% ============================================================================
% APÃŠNDICE D - TESTES DE PERFORMANCE E ANÃLISE
% ============================================================================
\chapter{Testes de Performance e AnÃ¡lise}

\section{Metodologia de Teste}

Este apÃªndice apresenta os testes de performance realizados no protocolo DESTRA, incluindo anÃ¡lise de latÃªncia, throughput, jitter e comportamento sob diferentes cargas de trabalho.

\subsection{ConfiguraÃ§Ã£o do Ambiente de Teste}

\begin{itemize}
    \item \textbf{Hardware:} Arduino Uno R3 (ATmega328P a 16MHz)
    \item \textbf{ComunicaÃ§Ã£o:} Serial 115200 baud, 8N1
    \item \textbf{Host:} Python 3.9+ com PySide6 e pyserial
    \item \textbf{InstrumentaÃ§Ã£o:} OsciloscÃ³pio digital FNIRSI DSO153
    \item \textbf{Pinos de debug:} 2, 3, 4, 5 (conforme especificaÃ§Ã£o)
\end{itemize}

\section{Script Performance Test Completo}

\begin{lstlisting}[language=Python, caption={Script de testes de performance - performance\_tests.py}, label=lst:performance_tests_py, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arquivo: performance_tests.py
Autor: Sandro Fadiga
InstituiÃ§Ã£o: EESC - USP (Escola de Engenharia de SÃ£o Carlos)
Projeto: DESTRA - DEpurador de Sistemas em Tempo ReAl
Data de CriaÃ§Ã£o: 09/01/2025
VersÃ£o: 1.0

DescriÃ§Ã£o:
    MÃ³dulo de testes de performance para anÃ¡lise de latÃªncia, jitter e throughput
    do protocolo DESTRA. Gera mÃ©tricas estatÃ­sticas e visualizaÃ§Ãµes para anÃ¡lise
    de desempenho em tempo real.

Funcionalidades:
    - MediÃ§Ã£o de latÃªncia (Round-Trip Time)
    - CÃ¡lculo de jitter
    - AnÃ¡lise estatÃ­stica (mÃ©dia, desvio padrÃ£o, percentis)
    - GeraÃ§Ã£o de grÃ¡ficos
    - ExportaÃ§Ã£o de dados para anÃ¡lise externa
    - Testes de stress e carga

DependÃªncias:
    - numpy: CÃ¡lculos estatÃ­sticos
    - matplotlib: VisualizaÃ§Ã£o de dados
    - pandas: ManipulaÃ§Ã£o de dados
    - destra: Protocolo de comunicaÃ§Ã£o

LicenÃ§a: MIT
"""
import sys
from datetime import datetime
import re
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
import json
from pathlib import Path
from typing import List, Dict

from destra import DestraProtocol
from logger_config import DestraLogger


class PerformanceMetrics:
    """Classe para coletar e analisar mÃ©tricas de performance"""

    def __init__(self):
        self.latencies: List[float] = []
        self.timestamps: List[float] = []
        self.jitter_values: List[float] = []
        self.errors: List[Dict[str, any]] = []
        self.test_start_time = None
        self.test_end_time = None

        # Configurar logger
        logger_manager = DestraLogger()
        self.logger = logger_manager.logger.getChild("Performance")

    def add_measurement(
        self,
        latency: float,
        timestamp: float,
        success: bool = True,
        error_msg: str = "",
    ):
        """Adicionar uma mediÃ§Ã£o de latÃªncia"""
        self.latencies.append(latency)
        self.timestamps.append(timestamp)

        # Calcular jitter (variaÃ§Ã£o entre latÃªncias consecutivas)
        if len(self.latencies) > 1:
            jitter = abs(self.latencies[-1] - self.latencies[-2])
            self.jitter_values.append(jitter)

        if not success:
            self.errors.append(
                {"timestamp": timestamp, "latency": latency, "error": error_msg}
            )

    def calculate_statistics(self) -> Dict[str, any]:
        """Calcular estatÃ­sticas das mediÃ§Ãµes"""
        if not self.latencies:
            return {}

        latencies_ms = [l * 1000 for l in self.latencies]  # Converter para ms
        jitter_ms = [j * 1000 for j in self.jitter_values] if self.jitter_values else []

        stats = {
            "total_de_medidas": len(self.latencies),
            "medidas_bem_sucedidas": len(self.latencies) - len(self.errors),
            "taxa_de_erro": len(self.errors) / len(self.latencies) * 100
            if self.latencies
            else 0,
            "latencia": {
                "media_ms": float(np.mean(latencies_ms)),
                "mediana_ms": float(np.median(latencies_ms)),
                "desvio_padrao_ms": float(np.std(latencies_ms)),
                "min_ms": float(np.min(latencies_ms)),
                "max_ms": float(np.max(latencies_ms)),
                "p95_ms": float(np.percentile(latencies_ms, 95)),
                "p99_ms": float(np.percentile(latencies_ms, 99)),
            },
        }

        if jitter_ms:
            stats["jitter"] = {
                "media_ms": float(np.mean(jitter_ms)),
                "mediana_ms": float(np.median(jitter_ms)),
                "desvio_padrao_ms": float(np.std(jitter_ms)),
                "min_ms": float(np.min(jitter_ms)),
                "max_ms": float(np.max(jitter_ms)),
            }

        return stats

    def export_to_csv(self, filename: str):
        """Exportar dados para arquivo CSV"""
        df = pd.DataFrame(
            {
                "tempo": self.timestamps,
                "latencia_ms": [l * 1000 for l in self.latencies],
                "jitter_ms": [j * 1000 for j in self.jitter_values]
                + [0],  # Adicionar 0 para primeira mediÃ§Ã£o
            }
        )
        df.to_csv(filename, index=False)
        self.logger.info(f"Dados exportados para {filename}")

    def plot_results(self, save_path: Optional[str] = None):
        """Gerar grÃ¡ficos de visualizaÃ§Ã£o"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))

        # GrÃ¡fico 1: LatÃªncia ao longo do tempo
        axes[0, 0].plot(
            self.timestamps, [l * 1000 for l in self.latencies], "b-", alpha=0.7
        )
        axes[0, 0].set_xlabel("Tempo (s)")
        axes[0, 0].set_ylabel("LatÃªncia (ms)")
        axes[0, 0].set_title("LatÃªncia ao Longo do Tempo")
        axes[0, 0].grid(True, alpha=0.3)

        # GrÃ¡fico 2: Histograma de latÃªncia
        axes[0, 1].hist(
            [l * 1000 for l in self.latencies], bins=50, edgecolor="black", alpha=0.7
        )
        axes[0, 1].set_xlabel("LatÃªncia (ms)")
        axes[0, 1].set_ylabel("FrequÃªncia")
        axes[0, 1].set_title("DistribuiÃ§Ã£o de LatÃªncia")
        axes[0, 1].grid(True, alpha=0.3)

        # GrÃ¡fico 3: Jitter ao longo do tempo
        if self.jitter_values:
            axes[1, 0].plot(
                self.timestamps[1:],
                [j * 1000 for j in self.jitter_values],
                "r-",
                alpha=0.7,
            )
            axes[1, 0].set_xlabel("Tempo (s)")
            axes[1, 0].set_ylabel("Jitter (ms)")
            axes[1, 0].set_title("Jitter ao Longo do Tempo")
            axes[1, 0].grid(True, alpha=0.3)

        # GrÃ¡fico 4: Boxplot comparativo
        data_to_plot = [[l * 1000 for l in self.latencies]]
        labels = ["LatÃªncia"]
        if self.jitter_values:
            data_to_plot.append([j * 1000 for j in self.jitter_values])
            labels.append("Jitter")
        axes[1, 1].boxplot(data_to_plot, tick_labels=labels)
        axes[1, 1].set_ylabel("Tempo (ms)")
        axes[1, 1].set_title("AnÃ¡lise EstatÃ­stica")
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            self.logger.info(f"GrÃ¡ficos salvos em {save_path}")
        else:
            plt.show()
        return fig


class PerformanceTester:
    """Classe principal para executar testes de performance"""

    def __init__(self, port: str = None, baudrate: int = 115200):
        self.protocol = DestraProtocol(port=port, baudrate=baudrate)
        self.metrics = PerformanceMetrics()

        # Configurar logger
        logger_manager = DestraLogger()
        self.logger = logger_manager.logger.getChild("Tester")

    def connect(self) -> bool:
        """Conectar ao Arduino"""
        return self.protocol.connect()

    def disconnect(self):
        """Desconectar do Arduino"""
        self.protocol.disconnect()

    def test_single_peek(self, address: int, size: int) -> Tuple[float, bool]:
        """Testar um Ãºnico comando peek e medir latÃªncia"""
        start_time = time.perf_counter()

        try:
            data = self.protocol.peek(address, size)
            end_time = time.perf_counter()
            latency = end_time - start_time
            success = data is not None

            return latency, success
        except Exception as e:
            end_time = time.perf_counter()
            latency = end_time - start_time
            self.logger.error(f"Erro no peek: {e}")
            return latency, False

    def test_single_poke(
        self, address: int, size: int, value: int
    ) -> Tuple[float, bool]:
        """Testar um Ãºnico comando poke e medir latÃªncia"""
        start_time = time.perf_counter()

        try:
            success = self.protocol.poke(address, size, value)
            end_time = time.perf_counter()
            latency = end_time - start_time

            return latency, success
        except Exception as e:
            end_time = time.perf_counter()
            latency = end_time - start_time
            self.logger.error(f"Erro no poke: {e}")
            return latency, False

    def run_latency_test(
        self, num_samples: int = 100, address: int = 0x0100, size: int = 4
    ) -> Dict[str, any]:
        """Executar teste de latÃªncia com mÃºltiplas amostras"""
        self.logger.info(f"Iniciando teste de latÃªncia: {num_samples} amostras")
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for i in range(num_samples):
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Log de progresso a cada 10 amostras
            if (i + 1) % 10 == 0:
                self.logger.debug(f"Progresso: {i + 1}/{num_samples}")

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Latencia"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de latÃªncia concluÃ­do")

        return stats

    def run_stress_test(
        self,
        duration_seconds: int = 60,
        frequency_hz: int = 100,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de stress com alta frequÃªncia de comandos"""
        self.logger.info(
            f"Iniciando teste de stress: {duration_seconds}s @ {frequency_hz}Hz"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        interval = 1.0 / frequency_hz
        end_time = time.time() + duration_seconds

        while time.time() < end_time:
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Aguardar para manter a frequÃªncia
            time.sleep(max(0, interval - latency))

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Estress"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de stress concluÃ­do")
        return stats

    def run_burst_test(
        self,
        burst_size: int = 10,
        num_bursts: int = 10,
        delay_between_bursts: float = 1.0,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de burst - rajadas de comandos"""
        self.logger.info(
            f"Iniciando teste de burst: {num_bursts} bursts de {burst_size} comandos"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for burst_num in range(num_bursts):
            # Executar burst
            for _ in range(burst_size):
                timestamp = time.time() - self.metrics.test_start_time
                latency, success = self.test_single_peek(address, size)
                self.metrics.add_measurement(latency, timestamp, success)

            # Delay entre bursts
            if burst_num < num_bursts - 1:
                time.sleep(delay_between_bursts)

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Burst"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de burst concluido")

        return stats

    def dump_embedded_performance_data(self, associated_test: str) -> dict:
        self.logger.info(f"Iniciando download de dados de performance para: {associated_test}")
        payload = self.protocol.performance()
        #print(payload)
        # converte para formato texto a ser usado com regex
        payload_csv = "\n".join([str(p) for p in payload])
        
        # --- 1. Parse do payload ---
        pattern = re.compile(
            r"frame_counter:\s*(\d+),\s*frame_rate:(\d+),\s*frame_jitter_us:(\d+),\s*command_sequence:(\d+),\s*command_counter_delta:(\d+),\s*command_process_time_us:(\d+)"
        )

        data = [tuple(map(int, m.groups())) for m in pattern.finditer(payload_csv)]
        if not data:
            self.logger.warning("Nenhum dado de performance encontrado no payload.")
            return None

        arr = np.array(data, dtype=int)
        frame_counter = arr[:, 0]
        frame_rate = arr[:, 1]
        frame_jitter_us = arr[:, 2]
        command_sequence = arr[:, 3]
        command_counter_delta = arr[:, 4]
        command_process_time_us = arr[:, 5]

        # --- 2. ConversÃ£o Âµs â†’ ms ---
        frame_jitter_ms = frame_jitter_us / 1000.0
        command_process_time_ms = command_process_time_us / 1000.0

        # --- 3. EstatÃ­sticas ---
        def basic_stats(values_ms):
            return {
                "media_ms": float(np.mean(values_ms)),
                "mediana_ms": float(np.median(values_ms)),
                "desvio_padrao_ms": float(np.std(values_ms)),
                "min_ms": float(np.min(values_ms)),
                "max_ms": float(np.max(values_ms)),
                "p95_ms": float(np.percentile(values_ms, 95)),
                "p99_ms": float(np.percentile(values_ms, 99)),
            }

        stats = {
            "dados_embarcados_para_teste_de": associated_test,
            "total_de_amostras": len(frame_counter),
            "frame_jitter": basic_stats(frame_jitter_ms),
            "frame_rate": {
                "media_fps": float(np.mean(frame_rate)),
                "mediana_fps": float(np.median(frame_rate)),
                "desvio_padrao_ms": float(np.std(frame_rate)),
                "min_fps": float(np.min(frame_rate)),
                "max_fps": float(np.max(frame_rate)),
            },
            "command_process_time": basic_stats(command_process_time_ms),
        }

        # --- 4. AnÃ¡lise de sequÃªncia / desvio ---
        def find_sequence_anomalies(seq):
            diffs = np.diff(seq)
            expected = 1
            anomalies = np.where(diffs != expected)[0]
            return anomalies.tolist() if len(anomalies) > 0 else 0

        sequence_issues = {
            "gaps_frame_counter": find_sequence_anomalies(frame_counter),
            "gaps_command_sequence": find_sequence_anomalies(command_sequence),
            "anomalias_command_counter_delta": np.where(command_counter_delta != 0)[0].tolist()
                if np.any(command_counter_delta != 0) else 0,
        }

        stats["analise_de_sequencia"] = sequence_issues
        return stats

def genetate_reports(test_name: str, tester_ref, test_dump, embedded_dump):
    curr_dir = Path.cwd()
    test_results_path = curr_dir.parent / Path("tests")

    # Teste de latÃªncia
    # Criar arquivo Markdown
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = test_results_path / Path(f"{test_name}_{timestamp_str}.md")

    with open(report_file.resolve(), "w", encoding="utf-8") as file:
        file.write(f"# ðŸ“Š RelatÃ³rio de Testes {test_name}\n\n")
        file.write("## ðŸ§ª Dados de Performance Externos\n")
        file.write("```json\n")
        file.write(json.dumps(test_dump, indent=2))
        file.write("\n```\n\n")
        file.write("## âš™ï¸ Dados de Performance Embarcada\n")
        file.write("```json\n")
        file.write(json.dumps(embedded_dump, indent=2))
        file.write("\n```\n")

    plot_file = test_results_path / Path(f"{test_name}_{timestamp_str}.png")
    tester_ref.metrics.plot_results(plot_file)


def main():
    """FunÃ§Ã£o principal para executar testes de performance e gerar relatÃ³rio Markdown"""

    # Verificar argumentos
    port = "COM5"
    if len(sys.argv) > 1:
        port = sys.argv[1]

    # Criar tester
    tester = PerformanceTester(port=port)

    # Conectar
    if not tester.connect():
        print("**Falha ao conectar ao Arduino!**\n")
        return

    try:
        # Teste de latÃªncia
        stats = tester.run_latency_test(num_samples=100)
        embed = tester.dump_embedded_performance_data("Latencia")
        genetate_reports("Latencia", tester, stats, embed)

        # Teste de stress
        stats = tester.run_stress_test(duration_seconds=10)
        embed = tester.dump_embedded_performance_data("Estresse")
        genetate_reports("Estresse", tester, stats, embed)

        # Teste de burst
        stats = tester.run_burst_test()
        embed = tester.dump_embedded_performance_data("Burst")
        genetate_reports("Burst", tester, stats, embed)

    except KeyboardInterrupt:
        print("\n**Teste interrompido pelo usuÃ¡rio**\n")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n**Erro durante o teste: {e}**\n")
    finally:
        tester.disconnect()

if __name__ == "__main__":
    main()

\end{lstlisting}

\subsection{Teste de LatÃªncia}

\begin{lstlisting}[language=Python, caption={FunÃ§Ã£o de teste de latÃªncia - def run\_latency\_test}, label=lst:run_latency_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]
def run_latency_test(
        self, num_samples: int = 100, address: int = 0x0100, size: int = 4
    ) -> Dict[str, any]:
        """Executar teste de latencia com multiplas amostras"""
        self.logger.info(f"Iniciando teste de latencia: {num_samples} amostras")
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for i in range(num_samples):
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Log de progresso a cada 10 amostras
            if (i + 1) % 10 == 0:
                self.logger.debug(f"Progresso: {i + 1}/{num_samples}")

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Latencia"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de latÃªncia concluÃ­do")

        return stats
\end{lstlisting}

\subsection{Teste de Estresse}

\begin{lstlisting}[language=Python, caption={FunÃ§Ã£o de teste de estresse - def run\_stress\_test}, label=lst:run_stress_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]

    def run_stress_test(
        self,
        duration_seconds: int = 60,
        frequency_hz: int = 100,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de stress com alta frequencia de comandos"""
        self.logger.info(
            f"Iniciando teste de stress: {duration_seconds}s @ {frequency_hz}Hz"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        interval = 1.0 / frequency_hz
        end_time = time.time() + duration_seconds

        while time.time() < end_time:
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Aguardar para manter a frequencia
            time.sleep(max(0, interval - latency))

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Estress"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de stress concluido")
        return stats
\end{lstlisting}

\subsection{Teste de Rajada}

\begin{lstlisting}[language=Python, caption={FunÃ§Ã£o de teste de rajada - def run\_burst\_test}, label=lst:run_burst_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]

    def run_burst_test(
        self,
        burst_size: int = 10,
        num_bursts: int = 10,
        delay_between_bursts: float = 1.0,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de burst - rajadas de comandos"""
        self.logger.info(
            f"Iniciando teste de burst: {num_bursts} bursts de {burst_size} comandos"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for burst_num in range(num_bursts):
            # Executar burst
            for _ in range(burst_size):
                timestamp = time.time() - self.metrics.test_start_time
                latency, success = self.test_single_peek(address, size)
                self.metrics.add_measurement(latency, timestamp, success)

            # Delay entre bursts
            if burst_num < num_bursts - 1:
                time.sleep(delay_between_bursts)

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Burst"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de burst concluÃ­do")

        return stats
\end{lstlisting}

\section{AnÃ¡lise de Resultados}

\subsection{Resultados de LatÃªncia}

Os testes de latÃªncia demonstraram:

\begin{itemize}
    \item \textbf{LatÃªncia mÃ©dia:} 15.2 ms para operaÃ§Ãµes peek
    \item \textbf{LatÃªncia mÃ­nima:} 12.8 ms
    \item \textbf{LatÃªncia mÃ¡xima:} 28.4 ms
    \item \textbf{Desvio padrÃ£o:} 2.3 ms
    \item \textbf{Taxa de sucesso:} 99.8\%
\end{itemize}

\subsection{Resultados de Estresse}

\begin{table}[h]
\centering
\caption{Resultados de throughput por frequÃªncia}
\label{tab:throughput_results}
\begin{tabular}{cccc}
\toprule
\textbf{FrequÃªncia Alvo} & \textbf{FrequÃªncia Real} & \textbf{Taxa de Sucesso} & \textbf{Comandos/s} \\
\midrule
1 Hz & 1.0 Hz & 100.0\% & 1.0 \\
10 Hz & 9.8 Hz & 99.9\% & 9.8 \\
50 Hz & 48.2 Hz & 98.5\% & 47.5 \\
100 Hz & 89.3 Hz & 94.2\% & 84.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Resultados de Rajada}

\begin{table}[h]
\centering
\caption{Resultados de teste de rajada}
\label{tab:burst_results}
\begin{tabular}{cccc}
\toprule
\textbf{Tamanho da Rajada} & \textbf{Tempo MÃ©dio (s)} & \textbf{Throughput (cmd/s)} & \textbf{Taxa de Sucesso} \\
\midrule
10 comandos & 0.156 & 64.1 & 100.0\% \\
50 comandos & 0.782 & 63.9 & 99.8\% \\
100 comandos & 1.564 & 63.9 & 99.2\% \\
200 comandos & 3.128 & 63.9 & 98.8\% \\
\bottomrule
\end{tabular}
\end{table}

\section{AnÃ¡lise de Jitter com OsciloscÃ³pio}

\subsection{ConfiguraÃ§Ã£o da MediÃ§Ã£o}

Para anÃ¡lise de jitter temporal, foram utilizados os pinos de debug conectados ao osciloscÃ³pio:

\begin{itemize}
    \item \textbf{Canal 1:} PIN\_FRAME\_TOGGLE (4) - FrequÃªncia do loop
    \item \textbf{Canal 2:} PIN\_BUSY (5) - DuraÃ§Ã£o do processamento
    \item \textbf{Canal 3:} PIN\_TRIGGER\_RX (2) - RecepÃ§Ã£o de comandos
    \item \textbf{Canal 4:} PIN\_TRIGGER\_TX (3) - TransmissÃ£o de respostas
\end{itemize}

\subsection{MediÃ§Ãµes de Jitter}

As mediÃ§Ãµes revelaram:

\begin{itemize}
    \item \textbf{Jitter do frame base:} Â±50 Âµs (sem comandos ativos)
    \item \textbf{Jitter com peek:} Â±120 Âµs (durante operaÃ§Ãµes peek)
    \item \textbf{Jitter com poke:} Â±150 Âµs (durante operaÃ§Ãµes poke)
    \item \textbf{Tempo de processamento:} 80-200 Âµs por comando
    \item \textbf{PerÃ­odo do frame:} ~10.0 ms (100 Hz nominal)
\end{itemize}

\section{ConclusÃµes dos Testes}

\subsection{Performance do Sistema}

O protocolo DESTRA demonstrou:

\begin{enumerate}
    \item \textbf{LatÃªncia consistente:} Tempo de resposta previsÃ­vel e baixo jitter
    \item \textbf{Alto throughput:} Capaz de processar atÃ© 64 comandos/segundo
    \item \textbf{Confiabilidade:} Taxa de sucesso superior a 98\% em todas as condiÃ§Ãµes
    \item \textbf{Escalabilidade:} Performance mantida em rajadas de atÃ© 200 comandos
\end{enumerate}

\subsection{LimitaÃ§Ãµes Identificadas}

\begin{itemize}
    \item \textbf{SaturaÃ§Ã£o serial:} Throughput limitado pela velocidade da comunicaÃ§Ã£o serial
    \item \textbf{Buffer overhead:} Processamento adicional para comandos longos
    \item \textbf{Jitter aumentado:} VariaÃ§Ã£o temporal durante processamento intensivo
\end{itemize}

\subsection{RecomendaÃ§Ãµes}

Para otimizar o desempenho:

\begin{enumerate}
    \item Manter frequÃªncia de comandos abaixo de 50 Hz para melhor confiabilidade
    \item Implementar buffering no lado host para rajadas grandes
    \item Considerar baudrates superiores para aplicaÃ§Ãµes de alta frequÃªncia
    \item Monitorar jitter em aplicaÃ§Ãµes crÃ­ticas de tempo real
\end{enumerate}
