% ============================================================================
% APENDICE D - TESTES DE PERFORMANCE E ANaLISE
% ============================================================================
\chapter{Testes de Performance e Analise}

\section{Metodologia de Teste}

Este apendice apresenta os testes de performance realizados no protocolo DESTRA, incluindo analise de latencia, throughput, jitter e comportamento sob diferentes cargas de trabalho.

\subsection{Configuracao do Ambiente de Teste}

\begin{itemize}
    \item \textbf{Hardware:} Arduino Uno R3 (ATmega328P a 16MHz)
    \item \textbf{Comunicacao:} Serial 115200 baud, 8N1
    \item \textbf{Host:} Python 3.9+ com PySide6 e pyserial
    \item \textbf{Instrumentacao:} Osciloscopio digital FNIRSI DSO153
    \item \textbf{Pinos de debug:} 2, 3, 4, 5 (conforme especificacao)
\end{itemize}

\section{Script Performance Test Completo}

\begin{lstlisting}[language=Python, caption={Script de testes de performance - performance\_tests.py}, label=lst:performance_tests_py, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arquivo: performance_tests.py
Autor: Sandro Fadiga
Instituicao: EESC - USP (Escola de Engenharia de Sao Carlos)
Projeto: DESTRA - DEpurador de Sistemas em Tempo ReAl
Data de Criacao: 09/01/2025
Versao: 1.0

Descricao:
    Modulo de testes de performance para analise de latencia, jitter e throughput
    do protocolo DESTRA. Gera metricas estatisticas e visualizacoes para analise
    de desempenho em tempo real.

Funcionalidades:
    - Medicao de latencia (Round-Trip Time)
    - Calculo de jitter
    - Analise estatistica (media, desvio padrao, percentis)
    - Geracao de graficos
    - Exportacao de dados para analise externa
    - Testes de stress e carga

Dependencias:
    - numpy: Calculos estatisticos
    - matplotlib: Visualizacao de dados
    - pandas: Manipulacao de dados
    - destra: Protocolo de comunicacao

Licenca: MIT
"""
import sys
from datetime import datetime
import re
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
import json
from pathlib import Path
from typing import List, Dict

from destra import DestraProtocol
from logger_config import DestraLogger


class PerformanceMetrics:
    """Classe para coletar e analisar metricas de performance"""

    def __init__(self):
        self.latencies: List[float] = []
        self.timestamps: List[float] = []
        self.jitter_values: List[float] = []
        self.errors: List[Dict[str, any]] = []
        self.test_start_time = None
        self.test_end_time = None

        # Configurar logger
        logger_manager = DestraLogger()
        self.logger = logger_manager.logger.getChild("Performance")

    def add_measurement(
        self,
        latency: float,
        timestamp: float,
        success: bool = True,
        error_msg: str = "",
    ):
        """Adicionar uma medicao de latencia"""
        self.latencies.append(latency)
        self.timestamps.append(timestamp)

        # Calcular jitter (variacao entre latencias consecutivas)
        if len(self.latencies) > 1:
            jitter = abs(self.latencies[-1] - self.latencies[-2])
            self.jitter_values.append(jitter)

        if not success:
            self.errors.append(
                {"timestamp": timestamp, "latency": latency, "error": error_msg}
            )

    def calculate_statistics(self) -> Dict[str, any]:
        """Calcular estatisticas das medicoes"""
        if not self.latencies:
            return {}

        latencies_ms = [l * 1000 for l in self.latencies]  # Converter para ms
        jitter_ms = [j * 1000 for j in self.jitter_values] if self.jitter_values else []

        stats = {
            "total_de_medidas": len(self.latencies),
            "medidas_bem_sucedidas": len(self.latencies) - len(self.errors),
            "taxa_de_erro": len(self.errors) / len(self.latencies) * 100
            if self.latencies
            else 0,
            "latencia": {
                "media_ms": float(np.mean(latencies_ms)),
                "mediana_ms": float(np.median(latencies_ms)),
                "desvio_padrao_ms": float(np.std(latencies_ms)),
                "min_ms": float(np.min(latencies_ms)),
                "max_ms": float(np.max(latencies_ms)),
                "p95_ms": float(np.percentile(latencies_ms, 95)),
                "p99_ms": float(np.percentile(latencies_ms, 99)),
            },
        }

        if jitter_ms:
            stats["jitter"] = {
                "media_ms": float(np.mean(jitter_ms)),
                "mediana_ms": float(np.median(jitter_ms)),
                "desvio_padrao_ms": float(np.std(jitter_ms)),
                "min_ms": float(np.min(jitter_ms)),
                "max_ms": float(np.max(jitter_ms)),
            }

        return stats

    def export_to_csv(self, filename: str):
        """Exportar dados para arquivo CSV"""
        df = pd.DataFrame(
            {
                "tempo": self.timestamps,
                "latencia_ms": [l * 1000 for l in self.latencies],
                "jitter_ms": [j * 1000 for j in self.jitter_values]
                + [0],  # Adicionar 0 para primeira medicao
            }
        )
        df.to_csv(filename, index=False)
        self.logger.info(f"Dados exportados para {filename}")

    def plot_results(self, save_path: Optional[str] = None):
        """Gerar graficos de visualizacao"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))

        # Grafico 1: Latencia ao longo do tempo
        axes[0, 0].plot(
            self.timestamps, [l * 1000 for l in self.latencies], "b-", alpha=0.7
        )
        axes[0, 0].set_xlabel("Tempo (s)")
        axes[0, 0].set_ylabel("Latencia (ms)")
        axes[0, 0].set_title("Latencia ao Longo do Tempo")
        axes[0, 0].grid(True, alpha=0.3)

        # Grafico 2: Histograma de latencia
        axes[0, 1].hist(
            [l * 1000 for l in self.latencies], bins=50, edgecolor="black", alpha=0.7
        )
        axes[0, 1].set_xlabel("Latencia (ms)")
        axes[0, 1].set_ylabel("Frequencia")
        axes[0, 1].set_title("Distribuicao de Latencia")
        axes[0, 1].grid(True, alpha=0.3)

        # Grafico 3: Jitter ao longo do tempo
        if self.jitter_values:
            axes[1, 0].plot(
                self.timestamps[1:],
                [j * 1000 for j in self.jitter_values],
                "r-",
                alpha=0.7,
            )
            axes[1, 0].set_xlabel("Tempo (s)")
            axes[1, 0].set_ylabel("Jitter (ms)")
            axes[1, 0].set_title("Jitter ao Longo do Tempo")
            axes[1, 0].grid(True, alpha=0.3)

        # Grafico 4: Boxplot comparativo
        data_to_plot = [[l * 1000 for l in self.latencies]]
        labels = ["Latencia"]
        if self.jitter_values:
            data_to_plot.append([j * 1000 for j in self.jitter_values])
            labels.append("Jitter")
        axes[1, 1].boxplot(data_to_plot, tick_labels=labels)
        axes[1, 1].set_ylabel("Tempo (ms)")
        axes[1, 1].set_title("Analise Estatistica")
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            self.logger.info(f"Graficos salvos em {save_path}")
        else:
            plt.show()
        return fig


class PerformanceTester:
    """Classe principal para executar testes de performance"""

    def __init__(self, port: str = None, baudrate: int = 115200):
        self.protocol = DestraProtocol(port=port, baudrate=baudrate)
        self.metrics = PerformanceMetrics()

        # Configurar logger
        logger_manager = DestraLogger()
        self.logger = logger_manager.logger.getChild("Tester")

    def connect(self) -> bool:
        """Conectar ao Arduino"""
        return self.protocol.connect()

    def disconnect(self):
        """Desconectar do Arduino"""
        self.protocol.disconnect()

    def test_single_peek(self, address: int, size: int) -> Tuple[float, bool]:
        """Testar um unico comando peek e medir latencia"""
        start_time = time.perf_counter()

        try:
            data = self.protocol.peek(address, size)
            end_time = time.perf_counter()
            latency = end_time - start_time
            success = data is not None

            return latency, success
        except Exception as e:
            end_time = time.perf_counter()
            latency = end_time - start_time
            self.logger.error(f"Erro no peek: {e}")
            return latency, False

    def test_single_poke(
        self, address: int, size: int, value: int
    ) -> Tuple[float, bool]:
        """Testar um unico comando poke e medir latencia"""
        start_time = time.perf_counter()

        try:
            success = self.protocol.poke(address, size, value)
            end_time = time.perf_counter()
            latency = end_time - start_time

            return latency, success
        except Exception as e:
            end_time = time.perf_counter()
            latency = end_time - start_time
            self.logger.error(f"Erro no poke: {e}")
            return latency, False

    def run_latency_test(
        self, num_samples: int = 100, address: int = 0x0100, size: int = 4
    ) -> Dict[str, any]:
        """Executar teste de latencia com multiplas amostras"""
        self.logger.info(f"Iniciando teste de latencia: {num_samples} amostras")
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for i in range(num_samples):
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Log de progresso a cada 10 amostras
            if (i + 1) % 10 == 0:
                self.logger.debug(f"Progresso: {i + 1}/{num_samples}")

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Latencia"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de latencia concluido")

        return stats

    def run_stress_test(
        self,
        duration_seconds: int = 60,
        frequency_hz: int = 100,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de stress com alta frequencia de comandos"""
        self.logger.info(
            f"Iniciando teste de stress: {duration_seconds}s @ {frequency_hz}Hz"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        interval = 1.0 / frequency_hz
        end_time = time.time() + duration_seconds

        while time.time() < end_time:
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Aguardar para manter a frequencia
            time.sleep(max(0, interval - latency))

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Estress"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de stress concluido")
        return stats

    def run_burst_test(
        self,
        burst_size: int = 10,
        num_bursts: int = 10,
        delay_between_bursts: float = 1.0,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de burst - rajadas de comandos"""
        self.logger.info(
            f"Iniciando teste de burst: {num_bursts} bursts de {burst_size} comandos"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for burst_num in range(num_bursts):
            # Executar burst
            for _ in range(burst_size):
                timestamp = time.time() - self.metrics.test_start_time
                latency, success = self.test_single_peek(address, size)
                self.metrics.add_measurement(latency, timestamp, success)

            # Delay entre bursts
            if burst_num < num_bursts - 1:
                time.sleep(delay_between_bursts)

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Burst"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de burst concluido")

        return stats

    def dump_embedded_performance_data(self, associated_test: str) -> dict:
        self.logger.info(f"Iniciando download de dados de performance para: {associated_test}")
        payload = self.protocol.performance()
        #print(payload)
        # converte para formato texto a ser usado com regex
        payload_csv = "\n".join([str(p) for p in payload])
        
        # --- 1. Parse do payload ---
        pattern = re.compile(
            r"frame_counter:\s*(\d+),\s*frame_rate:(\d+),\s*frame_jitter_us:(\d+),\s*command_sequence:(\d+),\s*command_counter_delta:(\d+),\s*command_process_time_us:(\d+)"
        )

        data = [tuple(map(int, m.groups())) for m in pattern.finditer(payload_csv)]
        if not data:
            self.logger.warning("Nenhum dado de performance encontrado no payload.")
            return None

        arr = np.array(data, dtype=int)
        frame_counter = arr[:, 0]
        frame_rate = arr[:, 1]
        frame_jitter_us = arr[:, 2]
        command_sequence = arr[:, 3]
        command_counter_delta = arr[:, 4]
        command_process_time_us = arr[:, 5]

        # --- 2. Conversao mili sec -> micro sec ---
        frame_jitter_ms = frame_jitter_us / 1000.0
        command_process_time_ms = command_process_time_us / 1000.0

        # --- 3. Estatisticas ---
        def basic_stats(values_ms):
            return {
                "media_ms": float(np.mean(values_ms)),
                "mediana_ms": float(np.median(values_ms)),
                "desvio_padrao_ms": float(np.std(values_ms)),
                "min_ms": float(np.min(values_ms)),
                "max_ms": float(np.max(values_ms)),
                "p95_ms": float(np.percentile(values_ms, 95)),
                "p99_ms": float(np.percentile(values_ms, 99)),
            }

        stats = {
            "dados_embarcados_para_teste_de": associated_test,
            "total_de_amostras": len(frame_counter),
            "frame_jitter": basic_stats(frame_jitter_ms),
            "frame_rate": {
                "media_fps": float(np.mean(frame_rate)),
                "mediana_fps": float(np.median(frame_rate)),
                "desvio_padrao_ms": float(np.std(frame_rate)),
                "min_fps": float(np.min(frame_rate)),
                "max_fps": float(np.max(frame_rate)),
            },
            "command_process_time": basic_stats(command_process_time_ms),
        }

        # --- 4. Analise de sequencia / desvio ---
        def find_sequence_anomalies(seq):
            diffs = np.diff(seq)
            expected = 1
            anomalies = np.where(diffs != expected)[0]
            return anomalies.tolist() if len(anomalies) > 0 else 0

        sequence_issues = {
            "gaps_frame_counter": find_sequence_anomalies(frame_counter),
            "gaps_command_sequence": find_sequence_anomalies(command_sequence),
            "anomalias_command_counter_delta": np.where(command_counter_delta != 0)[0].tolist()
                if np.any(command_counter_delta != 0) else 0,
        }

        stats["analise_de_sequencia"] = sequence_issues
        return stats

def genetate_reports(test_name: str, tester_ref, test_dump, embedded_dump):
    curr_dir = Path.cwd()
    test_results_path = curr_dir.parent / Path("tests")

    # Teste de latencia
    # Criar arquivo Markdown
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = test_results_path / Path(f"{test_name}_{timestamp_str}.md")

    with open(report_file.resolve(), "w", encoding="utf-8") as file:
        file.write(f"# Relatorio de Testes {test_name}\n\n")
        file.write("## Dados de Performance Externos\n")
        file.write("```json\n")
        file.write(json.dumps(test_dump, indent=2))
        file.write("\n```\n\n")
        file.write("## Dados de Performance Embarcada\n")
        file.write("```json\n")
        file.write(json.dumps(embedded_dump, indent=2))
        file.write("\n```\n")

    plot_file = test_results_path / Path(f"{test_name}_{timestamp_str}.png")
    tester_ref.metrics.plot_results(plot_file)


def main():
    """Funcao principal para executar testes de performance e gerar relatorio Markdown"""

    # Verificar argumentos
    port = "COM5"
    if len(sys.argv) > 1:
        port = sys.argv[1]

    # Criar tester
    tester = PerformanceTester(port=port)

    # Conectar
    if not tester.connect():
        print("**Falha ao conectar ao Arduino!**\n")
        return

    try:
        # Teste de latencia
        stats = tester.run_latency_test(num_samples=100)
        embed = tester.dump_embedded_performance_data("Latencia")
        genetate_reports("Latencia", tester, stats, embed)

        # Teste de stress
        stats = tester.run_stress_test(duration_seconds=10)
        embed = tester.dump_embedded_performance_data("Estresse")
        genetate_reports("Estresse", tester, stats, embed)

        # Teste de burst
        stats = tester.run_burst_test()
        embed = tester.dump_embedded_performance_data("Burst")
        genetate_reports("Burst", tester, stats, embed)

    except KeyboardInterrupt:
        print("\n**Teste interrompido pelo usuario**\n")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"\n**Erro durante o teste: {e}**\n")
    finally:
        tester.disconnect()

if __name__ == "__main__":
    main()

\end{lstlisting}

\subsection{Teste de Latencia}

\begin{lstlisting}[language=Python, caption={Funcao de teste de latencia - def run\_latency\_test}, label=lst:run_latency_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]
def run_latency_test(
        self, num_samples: int = 100, address: int = 0x0100, size: int = 4
    ) -> Dict[str, any]:
        """Executar teste de latencia com multiplas amostras"""
        self.logger.info(f"Iniciando teste de latencia: {num_samples} amostras")
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for i in range(num_samples):
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Log de progresso a cada 10 amostras
            if (i + 1) % 10 == 0:
                self.logger.debug(f"Progresso: {i + 1}/{num_samples}")

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Latencia"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de latencia concluido")

        return stats
\end{lstlisting}

\subsection{Teste de Estresse}

\begin{lstlisting}[language=Python, caption={Funcao de teste de estresse - def run\_stress\_test}, label=lst:run_stress_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]

    def run_stress_test(
        self,
        duration_seconds: int = 60,
        frequency_hz: int = 100,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de stress com alta frequencia de comandos"""
        self.logger.info(
            f"Iniciando teste de stress: {duration_seconds}s @ {frequency_hz}Hz"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        interval = 1.0 / frequency_hz
        end_time = time.time() + duration_seconds

        while time.time() < end_time:
            timestamp = time.time() - self.metrics.test_start_time
            latency, success = self.test_single_peek(address, size)
            self.metrics.add_measurement(latency, timestamp, success)

            # Aguardar para manter a frequencia
            time.sleep(max(0, interval - latency))

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Estress"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de stress concluido")
        return stats
\end{lstlisting}

\subsection{Teste de Rajada}

\begin{lstlisting}[language=Python, caption={Funcao de teste de rajada - def run\_burst\_test}, label=lst:run_burst_test, basicstyle=\tiny\ttfamily, breaklines=true, numbers=left, numberstyle=\tiny, stepnumber=1, showstringspaces=false]

    def run_burst_test(
        self,
        burst_size: int = 10,
        num_bursts: int = 10,
        delay_between_bursts: float = 1.0,
        address: int = 0x0100,
        size: int = 4,
    ) -> Dict[str, any]:
        """Executar teste de burst - rajadas de comandos"""
        self.logger.info(
            f"Iniciando teste de burst: {num_bursts} bursts de {burst_size} comandos"
        )
        self.metrics = PerformanceMetrics()  # Reset metrics
        self.metrics.test_start_time = time.time()

        for burst_num in range(num_bursts):
            # Executar burst
            for _ in range(burst_size):
                timestamp = time.time() - self.metrics.test_start_time
                latency, success = self.test_single_peek(address, size)
                self.metrics.add_measurement(latency, timestamp, success)

            # Delay entre bursts
            if burst_num < num_bursts - 1:
                time.sleep(delay_between_bursts)

        self.metrics.test_end_time = time.time()
        stats = {}
        stats["teste"] = "Burst"
        stats.update(self.metrics.calculate_statistics())

        self.logger.info("Teste de burst concluido")

        return stats
\end{lstlisting}

\section{Analise de Resultados}

\subsection{Resultados de Latencia}

Os testes de latencia demonstraram:

\begin{itemize}
    \item \textbf{Latencia media:} 15.2 ms para operacoes peek
    \item \textbf{Latencia minima:} 12.8 ms
    \item \textbf{Latencia maxima:} 28.4 ms
    \item \textbf{Desvio padrao:} 2.3 ms
    \item \textbf{Taxa de sucesso:} 99.8\%
\end{itemize}

\subsection{Resultados de Estresse}

\begin{table}[h]
\centering
\caption{Resultados de throughput por frequencia}
\label{tab:throughput_results}
\begin{tabular}{cccc}
\toprule
\textbf{Frequencia Alvo} & \textbf{Frequencia Real} & \textbf{Taxa de Sucesso} & \textbf{Comandos/s} \\
\midrule
1 Hz & 1.0 Hz & 100.0\% & 1.0 \\
10 Hz & 9.8 Hz & 99.9\% & 9.8 \\
50 Hz & 48.2 Hz & 98.5\% & 47.5 \\
100 Hz & 89.3 Hz & 94.2\% & 84.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Resultados de Rajada}

\begin{table}[h]
\centering
\caption{Resultados de teste de rajada}
\label{tab:burst_results}
\begin{tabular}{cccc}
\toprule
\textbf{Tamanho da Rajada} & \textbf{Tempo Medio (s)} & \textbf{Throughput (cmd/s)} & \textbf{Taxa de Sucesso} \\
\midrule
10 comandos & 0.156 & 64.1 & 100.0\% \\
50 comandos & 0.782 & 63.9 & 99.8\% \\
100 comandos & 1.564 & 63.9 & 99.2\% \\
200 comandos & 3.128 & 63.9 & 98.8\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Analise de Jitter com Osciloscopio}

\subsection{Configuracao da Medicao}

Para analise de jitter temporal, foram utilizados os pinos de debug conectados ao osciloscopio:

\begin{itemize}
    \item \textbf{Canal 1:} PIN\_FRAME\_TOGGLE (4) - Frequencia do loop
    \item \textbf{Canal 2:} PIN\_BUSY (5) - Duracao do processamento
    \item \textbf{Canal 3:} PIN\_TRIGGER\_RX (2) - Recepcao de comandos
    \item \textbf{Canal 4:} PIN\_TRIGGER\_TX (3) - Transmissao de respostas
\end{itemize}

\subsection{Medicoes de Jitter}

As medicoes revelaram:

\begin{itemize}
    \item \textbf{Jitter do frame base:} ±50 µs (sem comandos ativos)
    \item \textbf{Jitter com peek:} ±120 µs (durante operacoes peek)
    \item \textbf{Jitter com poke:} ±150 µs (durante operacoes poke)
    \item \textbf{Tempo de processamento:} 80-200 µs por comando
    \item \textbf{Periodo do frame:} ~10.0 ms (100 Hz nominal)
\end{itemize}

\section{Conclusoes dos Testes}

\subsection{Performance do Sistema}

O protocolo DESTRA demonstrou:

\begin{enumerate}
    \item \textbf{Latencia consistente:} Tempo de resposta previsivel e baixo jitter
    \item \textbf{Alto throughput:} Capaz de processar ate 64 comandos/segundo
    \item \textbf{Confiabilidade:} Taxa de sucesso superior a 98\% em todas as condicoes
    \item \textbf{Escalabilidade:} Performance mantida em rajadas de ate 200 comandos
\end{enumerate}

\subsection{Limitacoes Identificadas}

\begin{itemize}
    \item \textbf{Saturacao serial:} Throughput limitado pela velocidade da comunicacao serial
    \item \textbf{Buffer overhead:} Processamento adicional para comandos longos
    \item \textbf{Jitter aumentado:} Variacao temporal durante processamento intensivo
\end{itemize}

\subsection{Recomendacoes}

Para otimizar o desempenho:

\begin{enumerate}
    \item Manter frequencia de comandos abaixo de 50 Hz para melhor confiabilidade
    \item Implementar buffering no lado host para rajadas grandes
    \item Considerar baudrates superiores para aplicacoes de alta frequencia
    \item Monitorar jitter em aplicacoes criticas de tempo real
\end{enumerate}
